\documentclass[conference,compsoc]{IEEEtran}

\ifCLASSOPTIONcompsoc
  \usepackage[nocompress]{cite}
\else
  \usepackage{cite}
\fi
\newtheorem{theorem}{Defination}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{mathrsfs}
\floatname{algorithm}{algorithm} 
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{array}
\usepackage{url}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor}
\begin{document}

\title{Influence Maximization Problem}
\author{\IEEEauthorblockN{Wang Zhiyuan  11610634}
\IEEEauthorblockA{CSE\\
Computer Science and Technology\\
11610634@mail.sustc.edu.cn}}
\maketitle
\IEEEpeerreviewmaketitle 



\section{Preliminaries}
  \subsection{Software}
    In this project, I do some work to solve the Influence Maximization problem, which aid to find the initial seed set to let the final influence max in the social network.

    To do this work, I do it in two part, one is ISE(Influence Size Estimator), one is IMP(Influence Maximization Problems). And the social network we use is made in two models: IC(Independent Cascade) and LT(Linear Threshold).

    In the Project, I write the program by python and no extra package used. The test data is storaged in the txt. 

  \subsection{Algorithm}
    The algorithm I have came true is CELF and IMM. 
    The CELF is a greedy function with pruning, and the IMM is transfer the result of the active each round to the overlay of the RR(Reverse Reachable) set.
    
    In my test, the CElF can always get the best value in the IC model, but for the property of the LT, CELF can't always get the best result of the LT. But the largest dsiadvantage of the CELF is it's speed, when I apply CELF to a social network with 15k nodes and 30k edges. The CELF will work more than 2 hours.

    The IMM can work quickly and accuratly for both IC and LT. For the graph I mentioned last paragraph, IMM can work out in 20 seconds in both IC and LT for a 50 seeds initial set with $\epsilon = 0.1$, number of processing is 8.
    But IMM is not perfact, this algorithm will consume so much memory when handle a graph has so many nodes with a low $\epsilon$. For example, when I calculate a network with 425k nodes and $\epsilon = 0.1$, it need 13.2GiB memory.
    
\section{Methodology}
  \subsection{Representation}
    \subsubsection{ISE}
      In the ISE, I do it in 3 parts:
      \begin{itemize}
        \item $Build Map$: Read data from the file and generate the Adjacency \textbf{list}. Then reason that I choose the two dimension list but not the matrix or the dictionary is to get a balance of the memory ad the speed.
        \item $Create processing Pool$: Create a processing pool to do the multiprocessing to calculate quickly. 
        \item $Do IC or LT$: Calculate the result of the network and seed given for many time.
      \end{itemize}
    \subsubsection{CELF}
      In the CELF\cite{lv2014improved}\cite{goyal2011celf++}, I do it in 3 parts:
      \begin{itemize}
        \item $Build Map$: Read data from the file and generate the Adjacency \textbf{list}. Then reason that I choose the two dimension list but not the matrix or the dictionary is to get a balance of the memory ad the speed.
        \item $Do ISE$: Get the result of the seeds we choice in the given network and storage it in a \textbf{heap}
        \item $Choice nodes$: Choice that use the node in the top of the heap or do ISE again
      \end{itemize}
    \subsubsection{IMM}
      In the IMM, I do it in 3 parts:
      \begin{itemize}
        \item $Build Map$: Read data from the file and generate the Adjacency \textbf{list}. Then reason that I choose the two dimension list but not the matrix or the dictionary is to get a balance of the memory ad the speed.
        \item $Sampling$: Calculate the influence and create the \textbf{set} of RR set.
        \item $Node Selction$: Get the initial nodes set by compare the number of the RR set covered by the node.
      \end{itemize}
  \subsection{Architecture}
    \subsubsection{ISE}
      \begin{itemize}
        \item Read data and storage in memory.
        \item Get the active seeds initial.
        \item Active the node by the nodes actived last round.
        \item Do the last step until there are no new node actived in one round
      \end{itemize}
    \subsubsection{CELF}
      \begin{itemize}
        \item Read data and storage in memory.
        \item Calculate the influcence of the each nodes
        \item Choice that use the node on the top of the heap or Calculate the influence again
        \item Do the last step until the size of the set we get equal to the size we need
      \end{itemize}
    \subsubsection{IMM}
      \begin{itemize}
        \item Read data and storage in memory.
        \item Do the sample and generate the list of the RR set
        \item Do node selection for the list generated last step, get the final set
      \end{itemize}
  \subsection{Detail of Algorithm}
    \subsubsection{ISE}
      In the ISE, I do 10000 times estimate in 8 processing, each do 1250 times calculations. In each calculation, the program have two parts: LT and IC, choice which parts by the parameter input. The detail of two part can look at algorithm1 and algorithm2.
      \begin{algorithm}
        \caption{IC}
        \begin{algorithmic}[1]
          \Function{IC}{$nextNode, activeSet$}
            \State $activeNew\gets activeSet.copy()$
            \While{$activeNew$}
              \State $activeTemp \gets new\quad set()$
              \For{$i\quad in\quad activeNew$}
                \For{{$j\quad in \quad nextNode[i]$}}
                  \If{$random<j[1]$}
                    \If{$j[0]\quad not\quad in\quad activeSet$}
                      \State activeTemp.add[j[0]]
                      \State activeSet.add[j[0]]
                    \EndIf
                  \EndIf
                \EndFor
              \EndFor
              \State $activeNew = activeTemp.copy()$
            \EndWhile
            \Return $activeSet$
          \EndFunction
        \end{algorithmic}
      \end{algorithm}

      \begin{algorithm}
        \caption{LT}
        \begin{algorithmic}[1]
          \Function{LT}{$nodes, nextNode, activeSet$}
            \State $threshold\gets []$
            \For{$i\gets0\quad to nodes$}
              \State $threshold.append(random())$
            \EndFor
            \State $activeNew\gets activeSet.copy()$
            \While{activeNew}
              \State $activeTemp\gets new\quad set()$
              \For{$i\quad in\quad activeNew$}
                \For{$j\quad in\quad nextNode$}
                  \If{$j[0]\quad not\quad in\quad activeSet$}
                    \State $threshold[j[0]]-=j[1]$
                    \If{$threshold[j[0]]<=0$}
                      \State $activeTemp.add(j[0]$
                      \State $activeSet.add(j[0]$
                    \EndIf
                  \EndIf
                \EndFor
              \EndFor
              \State$activeNew\gets activeTemp.copy()$
            \EndWhile
            \Return $activeSet$
          \EndFunction
        \end{algorithmic}
      \end{algorithm}

    \subsubsection{CELF}
      For the CELF, I do once ISE for each node one time and storage them in a heap depend on the influcence increase of the node.
      Then, choice that using the node on the top of the heap or do the ISE for the nodes leftover.
      The detail of the CELF will be shown in the algorithm3.
      \begin{algorithm}
        \caption{CELF}
        \begin{algorithmic}[1]
          \Function{CELF}{$nodes, size$}
            \State $activeSet = set()$
            \State $que = PriorityQueue()$
            \For{$i\quad in\quad 0 \quad to\quad nodes$}
              \State $que.add(i, (ISE(i, activeSet)))$
            \EndFor
            \State $activeSet.add(que.get()[0])$
            \While{$len(activeSet)<size$}
              \State $B\gets que.get()$
              \If{$ISE(B[1], activeSet)<que.get()[1]$}
                \State $activeSet.add(B[0])$
              \Else
                \For{$i\quad in\quad 0 \quad to\quad nodes$}
                  \If{$i\quad not\quad in\quad activeSet$}
                    \State $que.add(i, (ISE(i, activeSet)))$
                  \EndIf
                \EndFor
              \EndIf
            \EndWhile
            \State \Return $activeSet$
          \EndFunction
        \end{algorithmic}
      \end{algorithm}
    \subsubsection{IMM}
      For the IMM, it translate the ability of the activing nodes to the number of the RR set can be covered.
      \begin{theorem}[Reverse Reachable Set]
        Let $v$ be a node in network $G$, and $g$ be a graph obtain by removing an each edge $e$ in $G$ with $1-p(e)$ probability. The reverse reachale(RR) set for $v$ in $g$ is the set of nodes in $g$ that can reach $v$.(That is, for each node $u$ in RR Set, there is a directed path from $u$ to $v$)
      \end{theorem}
      \begin{theorem}[Random RR set]
        Let $\mathcal{G}$ be the distribution of $g$ induced by the randomness in edge removal from $G$. A random RR set is an RR set generated on an instance of $g$ randomly sampled from $\mathcal{G}$, for a node selected uniformly at random from g.
      \end{theorem}
      After the defination of the RR set and random RR set, how IMM work? By the paper Influence Maximization: Near-Optimal Time Complexity Meets Practical Efficiency\cite{tang2015influence} we can know that if we add many enough RR set in the list R, and select the nodes that can cover the most number of the RR set, we must can get a good enough solution. But how many RR set we need to add in it? This is the work that IMM do more then the algorithm called TIM in the paper Influence Maximization: Near-Optimal Time Complexity Meets Practical Efficiency \cite{tang2014influence} do.
      First, we set some parameter that we will use in the sampling of the RR set.
      \begin{equation}        
        \lambda^{'} = \frac{(2+\frac{2}{3}\epsilon^{'})\cdot (\log(^{n}_{k})+l\cdot \log n+\log\log_{2}^{n})\cdot n}{\epsilon^{'2}}
      \end{equation}
      \begin{equation}
        \lambda^{*} = 2n\cdot ((1-\frac{1}{e})\cdot\alpha +\beta)^2\dot\epsilon^{-2}
      \end{equation}
      \begin{equation}
        \alpha = \sqrt{l\log n+log 2}
      \end{equation}
      \begin{equation}
        \beta = \sqrt{(1-\frac{1}{e})\cdot(\log(^n_k)+l\log n+ \log 2)}
      \end{equation}
      First of all, let's look at the algorithm the NodeSelection, in this part, we will get a given size set from the nodes in the G that let the nodes in the set can cover as more as possible.
      This part not only use in the last step to get the target set, but also use in the sampling part to determine the size of the RR set list we need.
      \begin{algorithm}
        \caption{NodeSelection}
        \begin{algorithmic}[1]
          \Function{NodeSelection}{$\mathcal{R},k$}
            \State $S_{k}^{*}\gets set()$
            \For{$i \gets 1\quad to\quad k$}
              \State $v\gets Maximizes\quad num(RR\quad Set\quad Covered)$
              \State $S_{k}^{*}.add(v)$
            \EndFor
          \EndFunction
          \State \Return $S_{k}^{*}$
        \end{algorithmic}
      \end{algorithm}
      In the NodeSelection part, how to let the speed of line 4 be highest is the key of the optimization of the algorithm.
      When I implment it, I use a \textbf{map} to storage many \textbf{list} as the \textbf{{value}}, each \textbf{list} storage the nodes that can be reached by the \textbf{key} node of this \textbf{list}.
      And at the same time, I count that the number of the nodes that can be reached for each \textbf{key} node
      Then, I will do the work that choice the node to add to the target set. I get the node that have the max number of the nodes can be reached, then update the map and the list that used to statistic last step.
      I will repeat this step until the target set is full.
      
      Then, I will do the Sampling part work. The detail of the sampling is in the algorithm5. 
      In the sampling, it calculate the number of the RR set needed Dynamicly and can save much time.
      \begin{algorithm}
        \caption{Sampling}
        \begin{algorithmic}[1]
          \Function{Sampling}{$G,k,\epsilon,l$}
            \State $\mathcal{R}\gets []$
            \State $LB\gets 1$
            \State $\epsilon^{'} \gets \sqrt{2}\cdot\epsilon$
            \For{$i \gets 1\quad to \quad \log_{2}{n}-1$}
              \State $x\gets n/2^i$
              \State $\Theta_i \gets \lambda^{'}/x$
              \While{$|\mathcal{R}|\leq \Theta_i$}
                \State $v\gets random(G.node)$
                \State $\mathcal{R}.append(v)$
              \EndWhile
              \State $S_i\gets NodeSelection(\mathcal{R},k)$
              \If{$n\cdot \mathcal{F_R}(S_i)\geq(1+\epsilon^{'}\cdot x$}
                \State $LB\gets n\cdot\mathcal{F_R}(S_i)/(1+\epsilon^{'}$
                \State $\textbf{break}$
              \EndIf
            \EndFor
            \State $\Theta\gets\lambda^{*}/LB$
            \While{$|\mathcal{R}|\leq \Theta$}
              \State $v\gets random(G.node)$
              \State $\mathcal{R}.append(v)$
            \EndWhile
            \State \Return $\mathcal{R}$
          \EndFunction
        \end{algorithmic}
      \end{algorithm}

\section{Empirical Verification}
  \subsection{Design}
    When I do the verification. I set the $hyperparameters$ $\epsilon = 0.1$, $l = 1$.
    \subsubsection{Network}: NetHEPT, Amazon and DBLP
    \subsubsection{Hyperparameter}: $\epsilon = 0.1\quad or\quad 0.2$, $l = 1$. The $\epsilon$ less, the speed lower, but the accuracy higher.
  \subsection{Performance}
    I test the program on my computer with AMD Ryzen 1700$3.6GHz@16$ and 16GB memory
    And the test for the NetHEPT use 16 processing and the test of the Amazon and DBLP will use 1 processing becausing The memory will not enough when calculate a very large network
    \begin{itemize}
      \item N: Num of nodes; E: Num of edges; M: Type of model
      \item S: Size of the seeds; P: Num of processing
      \item T: Time used; R: The result
    \end{itemize}
    \begin{table}[!htbp]
      \centering
      \caption{Result of test case}\label{tab:aStrangeTable}
      \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        \textbf{Network} & \textbf{N} & \textbf{E} & \textbf{M} & \textbf{S} & \textbf{$\epsilon$} &  \textbf{P} & \textbf{T} & \textbf{R}\\
        \hline
        NetHEPT&15233&32235&LT&50&0.1&16&13.83&1701.97\\
        \hline
        NetHEPT&15233&32235&IC&50&0.1&16&15.71&1295.52\\
        \hline
        NetHEPT&15233&32235&LT&50&0.2&16&4.33&1698.73\\
        \hline
        NetHEPT&15233&32235&IC&50&0.2&16&4.56&1290.56\\
        \hline
        Amazon&548526&13474&LT&50&0.2&1&189.55&852.99\\
        \hline
        Amazon&548526&13474&IC&50&0.2&1&347.62&636.59\\
        \hline
        Amazon&548526&13474&LT&50&0.5&1&70.15&847.83\\
        \hline
        Amazon&548526&13474&IC&50&0.5&1&69.76&628.03\\
        \hline
        DBLP&425746&15794&LT&50&0.1&1&376.17&1002.82\\
        \hline
        DBLP&425746&15794&LT&50&0.2&1&168.81&1001.21\\
        \hline
        DBLP&425746&15794&IC&50&0.2&1&181.13&658.26\\
        \hline
      \end{tabular}
    \end{table}
  \subsection{Result}
    From the table above, we can find that IMM have good effect to the IMP problem, but it work slowly in very large network with small $\epsilon$.
  \subsection{Analysis}
    For the test, I find that the IMM can work fast, but the performance is limited by the speed of node selction because it can not run in multiprocessing, so the next optimization step is add nodeselection into the multiprocessing.
    Another limitation of IMM is the memory, when a large number of nodes calculate with a low $\epsilon$, the number of RR set needed will be so large, and these RR set is required in the last node selection. So it will take so much memory, and the multiprocessing also can not be used.
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,mylib}

\end{document}


