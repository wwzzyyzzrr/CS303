\documentclass[conference,compsoc]{IEEEtran}

\ifCLASSOPTIONcompsoc
  \usepackage[nocompress]{cite}
\else
  \usepackage{cite}
\fi
\newtheorem{theorem}{Defination}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage{bm}
\floatname{algorithm}{algorithm} 
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{array}
\usepackage{url}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor}
\makeatletter
\newenvironment{breakablealgorithm}
  {% \begin{breakablealgorithm}
   \begin{center}
     \refstepcounter{algorithm}% New algorithm
     \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
     \renewcommand{\caption}[2][\relax]{% Make a new \caption
       {\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
       \ifx\relax##1\relax % #1 is \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
       \else % #1 is not \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
       \fi
       \kern2pt\hrule\kern2pt
     }
  }{% \end{breakablealgorithm}
     \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
   \end{center}
  }
\makeatother
\begin{document}

\title{Support Vector Machine}
\author{\IEEEauthorblockN{Wang Zhiyuan  11610634}
\IEEEauthorblockA{CSE\\
Computer Science and Technology\\
11610634@mail.sustc.edu.cn}}
\maketitle
\IEEEpeerreviewmaketitle 

\section{Preliminaries}
  \subsection{Software}
	In this project, I do some work to do the implement of the SVM\cite{cortes1995support} in two way, one is Gradient Descent\cite{bottou2010large} and another is SMO(Sequential Minimal Optimization)\cite{platt1998sequential}. 

    SVM is a very useful supervised learning model in the machine learning, it is used in many fields such as image recognization, object classification.

    Trainditional SVM have good effect in the case that the data is linear divisible, and if it need be used in the case that in the data is linear undivisible, it need use kernel function to process the data. In this report, the data I use will be linear divisible and the implement won`t include the kernel function.
  \subsection{Algorithm}
    \subsubsection{Overview}
        For the two algorithm I implement, the Gradient Descent is accroding to the loss function to do the Gradient Descent to reduce the loss.
        And the SMO is translating the problem $min\frac{1}{2}||\omega||^2$ to its dual problem $max\Sigma^{m}_{i=1}\alpha_i - \frac{1}{2} \Sigma^{m}_{i=1} \Sigma^{m}_{j=1}\alpha_i \alpha_j y_i y_j x_{i}^{T} x_j$
    \subsubsection{DataStructrue}
        In the Gradient Descent, I just use numpy.ndarray to storage the the data to do the training data and the predict model.
        And in the SMO, I use numpy.matrix to storage the training data and the model, use the list to storage the model of the dual problem. And the set is used in the select the first parameter that need to be optimized to storage that the parameter can not be optimized
\section{Methodology}
	\subsection{Representation}
			\subsubsection{Gradient Descent}
				In the Gradient Descent:
				\begin{itemize}
					\item Get the Gradient:
					\begin{equation}
						\left\{
							\begin{array}{lr}
								-y\times x & 1-y_i(<\bm{\omega},\bm{x_i}>+b)\leq 0\\
								0 & otherwise
							\end{array}
						\right.
					\end{equation}
				\item Do the moving
			\end{itemize}
			\subsubsection{SMO}
			In the smo:
			\begin{itemize}
				\item Calculate the $m\times m$ matrix K, using the linear kernel
				\item Random get the first $\alpha$, someone said that it should choice by the Heuristic function, but this way is too slow
				\item Get the second $\alpha$ by maximize the $|E_i-E_j|$
				\item Update the $\alpha_i$ and $\alpha_j$, E and b
			\end{itemize}
	\subsection{Architecture}
			\subsubsection{Gradient Descent}
				\begin{itemize}
					\item training
						\begin{itemize}
							\item $get\_loss$
							\item $move$
						\end{itemize}
					\item predict
				\end{itemize}
			\subsubsection{SMO}
				\begin{itemize}
					\item initialization
					\item training
						\begin{itemize}
							\item $select\_\alpha_i$
							\item $select\_\alpha_j$
							\item $update\_value$
						\end{itemize}
					\item predict
				\end{itemize}
	\subsection{Detail of Algorithm}
		\subsubsection{Gradiet Descent}
			First of all, we set two parameter: epochs and learningRate. 
			The epochs means the times that we will repeat the processing of learning.
			The learningRate means the step size of each moving. 

			Then we will repeat epochs times, and random the sequence of the training data.
			Then move the hyperplane opposed the direction of the Gradient

			The code of the algorithm is shown in the algorithm1.

			\begin{breakablealgorithm}
				\caption{Gradiet Desent}
				\begin{algorithmic}[1]
					\Function{GD}{$epochs, learningRate, size, \bm{w}$}
						\For{$i\quad from\quad 0\quad to\quad epochs$}
							\State $index\gets range(size)$
							\State $sort\quad index\quad randomly$
							\State $x\gets tran[index]$
							\State $y\gets label[index]$
							\State $loss\gets 0$
							\For{$xi,yi\quad in\quad zip(x,y)$}
								\State $loss\gets loss+max(0,1-y_i*\langle \bm{x_i},\bm{\omega}\rangle)$
								\If{$y*\langle\bm{x_i},\bm{\omega}\rangle<1$}
									\State $\bm{\omega}\gets \bm{\omega}+learningRate*y_i*\bm{x_i}$
								\EndIf
							\EndFor
							\If{$loss==0$}
								\State $\bm{Break}$
							\Else
								\State $Change\quad learnRate\quad by\quad loss$
							\EndIf
						\EndFor
					\EndFunction
				\end{algorithmic}
			\end{breakablealgorithm}
		\subsubsection{SMO}\cite{platt199912}
			The first step, we get the list of all the $\alpha_i$ that $0<\alpha<C$ and break the KKT, 
			if there is no $\alpha$ have $0<\alpha<C$ and break the KKT, 
			then append all $\alpha$ that $\alpha==0||\alpha==C$ and break KKT, 
			choice $\alpha_i$ in it randomly. 
			If the list is empty, break the repeat.

			The second step, for the $\alpha_i$ we choice, find all the $\alpha$ left to find the $\alpha_j$ to $max|E_i|$.
			Then we can calculate the range of the $\alpha_j$. If the $\alpha_j$ out of the range, let $\alpha_j \gets bound$, if $L>=H$, we will return false.
			The next operation is update the $\alpha_j$, in this operation, I need to get the $\eta$ of the $\alpha_i$ and $\alpha_j$, and then if the $\eta<0$, this $\eta$ is invalid and will return false.
			After update $\alpha_j$ successfully, we will update $\alpha_i$, E and b.

			We will do second step until all the $\alpha$ can not be optimized, or the process of update break down continuously for more than 500 times.
			
			Most of the code is shown in the algorithm 2 to 6, and there are also some equation of the K[i,j], E and b.
			\begin{equation}
				K[i,j] = \langle\bm{train[i]},\bm{train[j]}\rangle
			\end{equation}
			\begin{equation}
				E[i] = \langle\bm{\alpha}\times\bm{label}, \bm{K[i]}label\rangle + b - label[i]
			\end{equation}
			\begin{equation}
				\begin{split}
					b_i^{new} = -E_i-y_iK[i,i](\alpha_i^{new}-\alpha_i^{old})\\
					-y_jK[j,i](\alpha_j^{new}-\alpha_j^{old})+b^{old}
				\end{split}
			\end{equation}
			\begin{equation}
				\begin{split}
					b_j^{new} = -E_j-y_iK[i,j](\alpha_i^{new}-\alpha_i^{old})\\
					-y_jK[j,j](\alpha_j^{new}-\alpha_j^{old})+b^{old}
				\end{split}
			\end{equation}
			\begin{equation}
				\left\{
					\begin{array}{lr}
						b^{new}=b_i^{new}=b_j^{new} &0<\alpha_i{new}<C,0<\alpha_j^{new}<C \\
						\\
						b^{new}=\frac{b_i^{new}+b_j^{new}}{2} &otherwise\\
					\end{array}
				\right.
			\end{equation}
			$$$$
			\begin{breakablealgorithm}
				\caption{Update $\alpha_j$}
				\begin{algorithmic}[1]
					\Function{$update\alpha_j$}{$i,j,H,L$}
						\State $\eta\gets K[i,i]+k[j,j]-2*K[i,j]$
						\If{$\eta\leq 0$}
							\State \Return $False$
						\EndIf
						\State $\alpha[j]\gets \alpha[j]+\frac{label[i]*(e[i]-e[j])}{\eta}$
						\State $\alpha[j]\gets Let \quad \alpha[j] \quad not \quad out \quad Range$
						\State $updateE(j)$
					\EndFunction
				\end{algorithmic}
			\end{breakablealgorithm}
			$$$$
			$$$$
			\begin{breakablealgorithm}
				\caption{update $\alpha_i$}
				\begin{algorithmic}[1]
					\Function{$update\alpha_i$}{$i,j,\alpha_jOld$}
						\State $\alpha[i]\gets \alpha[i]+label[i]*label[j]*(\alpha_j-alpha_jOld)$
						\State $updateE(i)$
					\EndFunction
				\end{algorithmic}
			\end{breakablealgorithm}
			$$$$
			$$$$
			\begin{breakablealgorithm}
				\caption{Change $\alpha$}
				\begin{algorithmic}[1]
					\Function{changeAlpaha}{i}
						\State $E_i = e[i]$
						\If{$(label[i]*E_i<-toler\quad and \alpha[i]<C)$
						$or$
						$(label[i]*E_i>toler\quad and\quad \alpha[i]>0)$}
							\State $j,E_j\gets select\alpha_j(i,E_i)$
							\State $\alpha_jOld\gets \alpha[j].copy()$
							\If{$label[i]==label[j]$}
								\State $L\gets max(0,\alpha[i]+\alpha[j]-C)$
								\State $H\gets min(C,\alpha[i]+\alpha[j])$
							\Else
								\State $L\gets max(0,\alpha[j]-\alpha[j])$
								\State $L\gets max(C,C+\alpha[j]+\alpha[i])$								
							\EndIf
							\If{$L==H$}
								\State \Return $0$
							\EndIf
							\State $flag\gets update\alpha_j(i,j,H,L)$
							\If{$!flag$}
								\State \Return $0$
							\EndIf
							\State $update\alpha_i(i,j,\alpha_jOld)$
							\State $updateB(i,j)$
							\State \Return 1
						\Else
							\State \Return 0
						\EndIf
					\EndFunction
				\end{algorithmic}
			\end{breakablealgorithm}
			$$$$
			$$$$
			\begin{breakablealgorithm}
				\caption{select $\alpha_j$}
				\begin{algorithmic}[1]
					\Function{$selec\alpha_j$}{$i, Ei$}
						\State $max\gets 0$
						\State $index\gets -1$
						\For{$j\quad in\quad range(size)$}
							\State $temp\gets abs(e[j] - Ei)$
							\If{$temp>max$}
								\State $max\gets temp$
								\State $index\gets j$
							\EndIf
						\EndFor
					\If{$index==-1$}
						\State $index\gets random(size-1)$
						\While{$index==i$}
							\State $index\gets random(size-1)$
						\EndWhile
					\EndIf
					\State \Return $index, e[index]$
					\EndFunction
				\end{algorithmic}
			\end{breakablealgorithm}
			$$$$
			$$$$
			\begin{breakablealgorithm}
				\caption{SMO}
				\begin{algorithmic}[1]
					\Function{SMO}{$maxIter, C, toler, minMove, size$}
						\State $iters\gets 0$
						\State $notUpdate\gets 0$
						\State $over\gets False$
						\While{$iters<maxIter$}
							\State $index\gets set()$
							\For{$i\quad in\quad range(size)$}
								\If{$0<\alpha[i]<C$}
									\State $temp\gets abs(label[i]*g[i]-1)$
									\If{$temp>0$}
										\State $index.add(i)$
									\EndIf
								\EndIf
							\EndFor
							\If{$len(index)==0$}
								\For{$i\quad in\quad range(size)$}
									\If{$\alpha[i]==0$}
										\State $temp\gets 1-label[i]*g[i])$
										\If{$temp>0$}
											\State $index.add(i)$
										\EndIf
									\ElsIf{$\alpha[i]==0$}
										\State $temp\gets 1-label[i]*g[i])$
										\If{$temp>0$}
											\State $index.add(i)$
										\EndIf
									\EndIf
								\EndFor
							\EndIf
							\If{$len(index)==0$}
								\State $\bm{Break}$
							\EndIf
							\While{len(index)>0}
								\State $i\gets index.pop()$
								\State $flag = self.changeAlpha(i)$
								\If{$flag==0$}
									\State $notUpdate\gets notUpdate+1$
								\Else
									\State $notUpdate\gets 0$
									\State $iters\gets iters+1$
								\EndIf
								\If{$notUpdate>500$}:
									\State $over\gets True$
									\State $\bm{Break}$
								\EndIf
								\If{$iters>=maxIter$}
									\State $\bm{Break}$
								\EndIf
							\EndWhile
							\If{$over$}
								\State $\bm{Break}$
							\EndIf
						\EndWhile
					\EndFunction
				\end{algorithmic}
			\end{breakablealgorithm}

\section{Empirical Verification}
  \subsection{Design}
   In the verification of the SVM, I select the traing data on the sakai and divide it by $5:5$, $6:4$, $7:3$, $8:2$, $9:1$ as the training data and the test data.
   Then training and predict them by Gradient Descent and SMO and record the result.
  \subsection{Performance}
	The laptop I use is an Intel 6200U 2.8GHz@4 PC with 12GiB memory, for the test, it work just use battery, so the performance will be lower. 
	\subsubsection{Hyper paramters}
		In the Gradient Descent, I set the $epochs\gets 1000$, and the $learningRate\gets [0.05,0.01,0.005,0.001]$
		In the SMO, I set the $C\gets 200$, $iterTimes\gets 100$, $toler\gets 0.00001$ and $minMoving \gets 0.01$
  \subsection{Result}
	\begin{table}[h]
		\centering
		\caption{Result of Train data on sakai with linear kernel}\label{tab:aStrangeTable}
		\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\textbf{Algorithm} & \textbf{Train num} & \textbf{Test num} & \textbf{Iter Times} & \textbf{Wrong Rate} & \textbf{Time use}\\
		\hline
		GD	& 1200	& 134	& 1000	& 0.004	& 0.9\\ 
		\hline
		GD	& 1067	& 267	& 1000	& 0.002& 0.4\\
		\hline
		GD	& 933	& 301	& 1000	& 0.005 & 0.4\\
		\hline
		GD 	& 800	& 534	& 1000	& 0.0083& 0.26\\
		\hline
		GD	& 667	& 667	& 1000	& 0.0101& 0.2\\
		\hline
		SMO	& 1200	& 134	& 100	& 0.04	& 18\\ 
		\hline
		SMO	& 1067	& 267	& 100	& 0.04	& 15\\
		\hline
		SMO	& 933	& 301	& 100	& 0.04 & 11\\
		\hline
		SMO	& 800	& 534	& 100	& 0.055	& 9\\
		\hline
		SMO	& 667	& 667	& 100	& 0.05 & 7\\
		\hline
		\end{tabular}
	\end{table}
	The table 1 show the result that I test by the Gradient Descent and SMO.
  \subsection{Analysis}
	From the result in the table we can know, for a 10 demensions training data, the Gradient Descent can have great Correct Rate and high speed.
	
	But by the analysis of the code, we can find, in the Optimization processing of the SMO, the number of demensions of the training data nearly have no influence for the training speed, the speed of the SMO depend on the number of the training data.
	for the  Gradient Descent, we can find it calculate the $\bm{\omega}$ directly, so it will calculate slowly if there is a large number of demension training data, like when we de the recognization of image by Gradient Descent.
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,mylib}

\end{document}